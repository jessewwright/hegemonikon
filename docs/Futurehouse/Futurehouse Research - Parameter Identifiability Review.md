# Task
Please produce a comprehensive literature review on advanced methods for parameter estimation and model fitting in computational cognitive science, specifically for process models that generate choice probability and reaction time distributions. Focus on addressing common challenges:

# Response
Parameter Identifiability: Review techniques used when different parameter sets in a cognitive process model can produce very similar predicted behavioral data (choices and RTs), making parameters difficult to estimate uniquely. Discuss methods for diagnosing and mitigating identifiability issues.
Hierarchical Bayesian Modeling (HBM): Detail the application of HBM to fit cognitive process models across individuals or conditions. Discuss advantages (e.g., handling individual differences, incorporating priors, borrowing strength) and common implementation platforms (e.g., PyMC, Stan). Include discussion on the use and challenges of defining appropriate likelihood functions for stochastic cognitive processes within HBM frameworks.
Joint Likelihoods: Review the rationale and methods for fitting models simultaneously to multiple types of behavioral data (e.g., combining choice accuracy and the full distribution of reaction times) to better constrain parameter estimates.
Fitting Models with Latent Dynamics: Discuss approaches for estimating parameters in models where internal states or parameters might change over time or trials based on feedback or internal calculations.
Provide a structured report covering methodologies, their strengths and weaknesses, examples of applications to decision-making or cognitive control models, and common pitfalls encountered in practice.

Below is a comprehensive review of advanced methods for parameter estimation and model fitting in computational cognitive science, with a focus on process models that generate both choice probability and reaction time (RT) distributions. The report is organized into four main sections: (1) Parameter Identifiability, (2) Hierarchical Bayesian Modeling (HBM), (3) Joint Likelihoods for Simultaneous Fitting of Multiple Data Streams, and (4) Fitting Models with Latent Dynamics. Throughout, examples from decision‐making and cognitive control models are used to illustrate the methodologies, their strengths and weaknesses, and common pitfalls encountered in practice.

## 1. Introduction

Computational cognitive science relies on rigorous process models to capture the dynamics underlying choice behavior and response times, which are assumed to emanate from internal cognitive processes. These models—ranging from evidence accumulation models like the diffusion decision model to more complex frameworks incorporating latent states—aim to quantify psychological constructs such as learning rates, thresholds, and biases (ballard2019jointmodelingof pages 1-4). However, a recurring challenge in model fitting is parameter identifiability, whereby different parameter sets yield nearly equivalent predictions for observable behavior (ballard2019jointmodelingof pages 13-16). Moreover, many cognitive models include latent dynamics that evolve over time, requiring advanced estimation techniques that can handle stochastic likelihoods and simultaneous heterogeneous data sources, such as choices and RT distributions (molloy2019what’sina pages 1-5).

Recent advances in Bayesian methods, particularly hierarchical Bayesian modeling (HBM), have dramatically expanded the toolbox for cognitive modelers, enabling the explicit incorporation of prior knowledge and systematic handling of individual differences (lee2011howcognitivemodeling pages 1-2, baribault2019usinghierarchicalbayesian pages 12-16). In addition, joint likelihood approaches have been proposed so as not to model only choices or RTs in isolation but to simultaneously fit the entire distribution of RTs along with choice outcomes (rouder2015thelognormalrace pages 1-3). This integration not only constrains parameter estimates more effectively but also provides deeper insight into the underlying cognitive processes by exploiting complementary information from multiple data streams (evans2019responsetimedataprovide pages 3-5). Finally, the challenge of fitting models with latent dynamics—where internal parameters change with accumulating evidence or feedback—necessitates careful methodological considerations in both simulation procedures and Markov chain Monte Carlo (MCMC) inference (murrow2024modelsandmethods pages 78-80). In the following sections, we review these core challenges and the state-of-the-art approaches employed to overcome them.

## 2. Parameter Identifiability

Parameter identifiability refers to the unique determination of model parameters based on observed behavioral data. In many cognitive process models, such as reinforcement learning or evidence accumulation models, multiple parameter combinations can yield virtually identical or highly similar predictions for choice and RT outcomes (ballard2019jointmodelingof pages 1-4). This ambiguity poses severe limitations in interpreting parameter estimates, as overparametrized or “sloppy” models may fail to provide conclusive evidence regarding the underlying cognitive processes (ballard2019jointmodelingof pages 13-16).

To diagnose identifiability issues, simulation-based recovery studies have become a standard tool. In these studies, synthetic datasets are generated using known parameter values, and estimation procedures are subsequently applied to check if the model can recover the original parameters (ratcliff2015individualdifferencesand pages 46-49). Correlations between the known “true” parameters and those recovered from the simulated data serve as a diagnostic metric, with high correlations indicating better identifiability (ratcliff2015individualdifferencesand pages 49-53). Likewise, some researchers introduce specific diagnostic metrics such as Drift Error (DE) which quantifies the discrepancy between the true and estimated drift rate functions in accumulation models (murrow2024modelsandmethods pages 102-104). A low DE score suggests effective parameter recovery, whereas a high DE indicates that different parameter sets can produce similar likelihood profiles, thereby compromising identifiability (murrow2024modelsandmethods pages 104-107).

Additional strategies to improve identifiability include imposing theoretically motivated constraints and reparameterizing models so that intrinsic trade-offs between parameters are minimized (heck2016extendingmultinomialprocessing pages 6-8). For instance, in response time–extended multinomial processing tree (MPT) models, grouping response categories based on latent processing branches can isolate parameters that are directly estimable, thereby mitigating identifiability issues (heck2016extendingmultinomialprocessing pages 18-19). Similarly, when multiple parameters are highly correlated, using prior predictive checks helps determine if additional behavioral data streams, such as RT distributions, provide extra constraints that shrink the viable parameter space (ballard2019jointmodelingof pages 19-22, baribault2023troubleshootingbayesiancognitive pages 40-43).

While these diagnostic approaches are valuable, they often require careful design of simulation studies, appropriate choice of prior distributions, and detailed investigation of likelihood surfaces across parameter spaces (baribault2023troubleshootingbayesiancognitive pages 53-57). In some cases, even with additional constraints, estimated parameters may display residual uncertainties or shrinkage effects that need further exploration through robust model checking and replications across different experimental conditions (ratcliff2015individualdifferencesand pages 53-56).

## 3. Hierarchical Bayesian Modeling (HBM)

Hierarchical Bayesian modeling (HBM) has emerged as a powerful framework in cognitive modeling, particularly for handling data collected from multiple subjects or experimental conditions (lee2011howcognitivemodeling pages 1-2, baribault2019usinghierarchicalbayesian pages 12-16). The central idea behind HBM is to introduce multiple levels of parameters: individual-level parameters govern subject-specific cognitive processes, while group-level hyperparameters capture shared tendencies across subjects (lee2011howcognitivemodeling pages 3-4). This strategy borrows strength from the entire dataset, thereby leading to improved precision in parameter estimates even when data are sparse on an individual level.

One of the principal advantages of HBM is its natural incorporation of individual differences. By modeling each subject’s parameters as random effects drawn from a group-level distribution, cognitive models can account for variability across participants while simultaneously estimating overall population tendencies (baribault2019usinghierarchicalbayesian pages 38-42, lee2018bayesianmethodsin pages 1-3). This partial pooling mechanism mitigates overfitting and improves stability, particularly in scenarios where each individual contributes only a limited number of trials (lee2011howcognitivemodeling pages 6-6).

Implementation platforms such as PyMC, Stan, and JAGS are commonly used for specifying and fitting hierarchical cognitive models. These probabilistic programming languages facilitate efficient MCMC sampling to approximate the full posterior distributions over both individual-level and hyperparameters (fengler2023likelihoodapproximationsfor pages 34-36, lee2018bayesianmethodsin pages 3-5). The flexibility inherent in these platforms also allows modelers to incorporate informative priors based on theoretical constraints or previous empirical findings, which further guides the estimation process and helps constrain the parameter space (baribault2023troubleshootingbayesiancognitive pages 1-6, lee2018bayesianmethodsin pages 31-33).

Despite its many strengths, HBM also presents challenges. One key issue is the specification of appropriate prior distributions, which must be carefully chosen to avoid overly diffuse or overly constraining estimates that might bias the results (baribault2023troubleshootingbayesiancognitive pages 49-53). In addition, hierarchical models can be computationally intensive, particularly when the underlying likelihood functions are complex or when the models involve non-linear stochastic processes (fengler2023likelihoodapproximationsfor pages 41-46). In practice, advanced diagnostic techniques – such as monitoring convergence statistics including the Gelman-Rubin R̂, checking effective sample sizes, and visual inspection of trace plots – are essential to ensure reliable inference (baribault2023troubleshootingbayesiancognitive pages 6-9, lee2018bayesianmethodsin pages 31-33).

## 4. Joint Likelihoods for Simultaneous Fitting of Multiple Data Streams

Rather than fitting cognitive models solely to choice data or RT distributions individually, the joint likelihood approach advocates for simultaneous modeling of both data streams to exploit their complementary information (ballard2019jointmodelingof pages 1-4). The rationale behind joint likelihood modeling is that while choice outcomes provide discrete data reflecting ultimate decisions, RT distributions capture the temporal dynamics of the decision process, thereby offering additional constraints on the model parameters (ballard2019jointmodelingof pages 19-22, rouder2015thelognormalrace pages 1-3).

By jointly modeling both choice and RT data, one can reduce the uncertainty associated with each individual parameter. For instance, in reinforcement learning models, incorporating RT data alongside choice outcomes can halve the estimation variance on learning rate parameters relative to fits using choice data alone (ballard2019jointmodelingof pages 1-4). In evidence accumulation models, joint likelihoods allow the estimation procedure to capture not only which alternative was chosen but also the speed of evidence accumulation, providing a nuanced assessment of cognitive processes like drift rate, boundary separation, and nondecision time (evans2019responsetimedataprovide pages 30-32, rouder2015thelognormalrace pages 3-5).

The lognormal race model serves as one illustrative example. This model is designed to account for multiple response options by assuming that each choice is determined by the first accumulator that reaches its threshold, with finishing times modeled as lognormal variables (rouder2015thelognormalrace pages 1-3). Its parametric assumptions are not only well suited to the skewed nature of RT data, but also allow the model to flexibly incorporate covariates and latent variables via standard psychometric or IRT-like structures (rouder2015thelognormalrace pages 3-5). In addition, other models such as the Multi-attribute Linear Ballistic Accumulator (MLBA) extend the joint likelihood framework to more complex decision contexts where context effects (e.g., compromise, similarity, and attraction effects) are systematically studied (molloy2019what’sina pages 1-5).

Despite its appeal, joint likelihood modeling also has its challenges. One critical pitfall is that the models require the derivation or approximation of a combined likelihood function that appropriately captures the statistical dependencies between choices and RTs, which can be highly non-trivial if the processes generating these data are complex (ballard2019jointmodelingof pages 13-16). In many cases, simulation-based methods or approximate Bayesian computation techniques are employed to circumvent analytic intractability (evans2019responsetimedataprovide pages 23-24). The resulting computational demands are higher compared to simpler fitting procedures based solely on one data type; however, the improved constraint on parameters often justifies these extra efforts (ballard2019jointmodelingof pages 1-4).

## 5. Fitting Models with Latent Dynamics

A further layer of complexity is introduced when models incorporate latent dynamics—situations in which internal parameters or states change over time or trials. Such models are typical in reinforcement learning and cognitive control, where the learning rate, bias, or value estimates continuously evolve in response to feedback and experience (ballard2019jointmodelingof pages 13-16). Fitting these models requires not only traditional estimation of static parameters but also dynamic tracking of parameters as they update with each trial.

In these contexts, dynamic models are often implemented using sequential formulations like stochastic differential equations (SDEs) or time-varying latent process models (murrow2024modelsandmethods pages 31-34). For instance, the Drift Diffusion Model (DDM) and its extensions model decision making as an evidence accumulation process which is influenced by time-varying internal states and external stimuli (heathcote2019dynamicmodelsof pages 1-2). To capture these dynamics, one must compute the full likelihood of a sequence of responses, accounting for trial-to-trial fluctuations and nonstationarities in the cognitive process.

Techniques for fitting models with latent dynamics include maximum likelihood estimation methods that incorporate numerical methods such as the Euler-Maruyama integration method for simulating evidence trajectories, combined with normal approximations to the evolving likelihood functions (murrow2024modelsandmethods pages 78-80). Often, these approaches are embedded within hierarchical Bayesian frameworks in order to capture individual learning curves and dynamic changes in internal processing (lu2025hierarchicalbayesianaugmented pages 11-14, murrow2024modelsandmethods pages 104-107). The Bayesian approach is particularly well suited for these models because it can naturally accommodate the uncertainty in the estimated dynamic trajectories via full posterior distributions that are updated as new data are observed.

One must note, however, that computational challenges here are even more pronounced. The need to simulate high-dimensional stochastic processes and to evaluate complex, time-indexed likelihood functions results in long computation times and potentially challenging convergence properties in MCMC sampling (fengler2023likelihoodapproximationsfor pages 41-46, murrow2024modelsandmethods pages 104-107). To address these issues, recent work has focused on integrating acceleration technologies such as PyTensor (for GPU-based computation) and using feature engineering to construct differentiable likelihood approximations, which considerably reduce computational overhead while maintaining estimation accuracy (lu2025hierarchicalbayesianaugmented pages 5-8).

## 6. Methodological Strengths, Weaknesses, and Applications

Each of the advanced methods reviewed herein comes with inherent strengths and trade-offs. Parameter identifiability diagnostics, such as simulation-based recovery studies and drift error analyses, are indispensable for verifying that the estimated parameters truly reflect underlying cognitive processes rather than artefacts of overparameterization (murrow2024modelsandmethods pages 102-104, heck2016extendingmultinomialprocessing pages 6-8). While these techniques can reveal weaknesses in model design, they also suggest routes for improvement—such as reparameterization or strategic imposition of theoretically grounded constraints—which in turn can lead to more robust and interpretable models (baribault2023troubleshootingbayesiancognitive pages 40-43).

Hierarchical Bayesian models stand out for their capacity to handle individual differences and to incorporate prior information systematically. Their partial pooling mechanism often leads to improved parameter precision, especially when individual data are sparse. However, modelers must be mindful of issues in specifying priors and ensuring convergence of complex models; poor choices can lead to biased estimates or slow convergence in MCMC procedures (lee2011howcognitivemodeling pages 6-6, baribault2023troubleshootingbayesiancognitive pages 1-6).

Joint likelihood approaches provide a significant advantage in that they leverage the full structure of the available behavioral data by jointly fitting choice probabilities and RT distributions. This simultaneous treatment provides extra constraints on parameter estimates enabling better discrimination among competing cognitive theories, as demonstrated by both reinforcement learning models and evidence accumulation models (ballard2019jointmodelingof pages 1-4, rouder2015thelognormalrace pages 1-3). Nonetheless, fitting joint likelihood models remains computationally intensive, and researchers must frequently resort to simulation-based likelihood approximations when closed-form solutions are unavailable (evans2019responsetimedataprovide pages 23-24).

Fitting models with latent dynamics represents a frontier where the complexity of cognitive processes is embraced rather than simplified away. These approaches enable the modeling of learning and changing internal states over time and can capture phenomena such as adaptation, fatigue, or strategic shifts in decision making (murrow2024modelsandmethods pages 31-34, ballard2019jointmodelingof pages 13-16). Yet, the increased flexibility of such models comes at a cost. The computational burden, coupled with the risk of overfitting or misestimating dynamic changes due to noise, requires rigorous diagnostic testing, robust simulation procedures, and often strategic sacrifices in model complexity to maintain tractability (fengler2023likelihoodapproximationsfor pages 41-46).

Applications of these advanced methods are found across a broad spectrum of cognitive research. For example, in studies of reinforcement learning, joint modeling of RT and choice data has been shown to substantially improve the identifiability of learning rate parameters, which can then be more reliably linked to neural indices such as dopaminergic prediction error signals (ballard2019jointmodelingof pages 1-4). In the domain of perceptual decision making, models like the lognormal race and diffusion models have been extensively employed to understand processing under time pressure, with hierarchical Bayesian approaches facilitating the comparison of individual differences in nondecision times, boundary separations, and drift rates (rouder2015thelognormalrace pages 1-3, heathcote2019dynamicmodelsof pages 6-7).

From a psychometric perspective, joint likelihood models have allowed for the embedding of cognitive process parameters within Item Response Theory (IRT) frameworks, thereby linking detailed cognitive mechanisms with broader assessments of ability and performance (kyllonen2016useofresponse pages 13-16). Similarly, hierarchical approaches in cognitive modeling have been harnessed in educational and clinical contexts to distinguish between normal cognitive variability and deficits—provided that the underlying parameter estimates are both reliable and valid (ratcliff2015individualdifferencesand pages 46-49).

Common pitfalls in these methodologies include underestimating the complexity introduced by contaminants and noise (such as fast guesses or lapses of attention), which can lead to overfitting and erroneous parameter trade-offs (ratcliff2015individualdifferencesand pages 53-56). Moreover, while hierarchical models offer great promise, they require careful convergence diagnostics and may demand modifications such as non-centered parameterization to address pathological posterior geometries (baribault2023troubleshootingbayesiancognitive pages 53-57). Finally, the increased computational burden of joint likelihood and latent dynamics models means that practical implementations must often strike a balance between richness of representation and computational feasibility (fengler2023likelihoodapproximationsfor pages 41-46, murrow2024modelsandmethods pages 78-80).

## 7. Conclusion

Advanced methods for parameter estimation and model fitting in computational cognitive science now provide researchers with powerful tools to probe the complexities of human decision-making. Addressing parameter identifiability challenges through simulation-based recovery, reparameterization, and constraint imposition is critical to ensure that the models meaningfully reflect underlying processes (ballard2019jointmodelingof pages 13-16, heck2016extendingmultinomialprocessing pages 6-8). Hierarchical Bayesian modeling stands as a robust framework for capturing individual differences and integrating prior knowledge, albeit with challenges in prior specification and computational demand (lee2011howcognitivemodeling pages 1-2, baribault2023troubleshootingbayesiancognitive pages 1-6). Joint likelihood approaches that combine choice and reaction time data enhance the constraints on parameter estimates, thereby improving model discrimination and interpretability (rouder2015thelognormalrace pages 1-3, ballard2019jointmodelingof pages 19-22). Finally, fitting models with latent dynamics—those that account for evolving internal states—requires sophisticated simulation methods, full likelihood estimation, and state-of-the-art computational tools to manage the attendant complexity (murrow2024modelsandmethods pages 78-80, lu2025hierarchicalbayesianaugmented pages 11-14).

While significant progress has been made in deploying these advanced methodologies, common pitfalls remain. Researchers must remain vigilant regarding potential overfitting, ensure robust diagnostic testing for identifiability, and carefully balance model sophistication with computational feasibility. Continued innovation in probabilistic programming, accelerated computing (e.g., via GPU-enabled libraries such as PyTensor), and integration of simulation-based diagnostic techniques promises to further enhance the reliability and applicability of cognitive models in both laboratory and applied settings (fengler2023likelihoodapproximationsfor pages 41-46, murrow2024modelsandmethods pages 104-107).

In summary, the convergence of improved identifiability diagnostics, hierarchical Bayesian frameworks, joint likelihood approaches, and methods for dynamically evolving latent parameters represents the leading edge of computational cognitive science. These techniques not only allow for more precise parameter recovery and better theoretical discrimination but also open the door to richer applications in understanding, predicting, and ultimately intervening in human cognitive function (ballard2019jointmodelingof pages 1-4, lee2018bayesianmethodsin pages 1-3, rouder2015thelognormalrace pages 1-3).

## References

1. (ballard2019jointmodelingof pages 1-4): Ian C. Ballard and Samuel M. McClure. Joint modeling of reaction times and choice improves parameter identifiability in reinforcement learning models. Journal of Neuroscience Methods, 317:37-44, Apr 2019. URL: https://doi.org/10.1101/306720, doi:10.1101/306720. This article has 56 citations and is from a peer-reviewed journal.

2. (ballard2019jointmodelingof pages 13-16): Ian C. Ballard and Samuel M. McClure. Joint modeling of reaction times and choice improves parameter identifiability in reinforcement learning models. Journal of Neuroscience Methods, 317:37-44, Apr 2019. URL: https://doi.org/10.1101/306720, doi:10.1101/306720. This article has 56 citations and is from a peer-reviewed journal.

3. (ballard2019jointmodelingof pages 19-22): Ian C. Ballard and Samuel M. McClure. Joint modeling of reaction times and choice improves parameter identifiability in reinforcement learning models. Journal of Neuroscience Methods, 317:37-44, Apr 2019. URL: https://doi.org/10.1101/306720, doi:10.1101/306720. This article has 56 citations and is from a peer-reviewed journal.

4. (baribault2019usinghierarchicalbayesian pages 12-16): B Baribault. Using hierarchical bayesian models to test complex theories about the nature of latent cognitive processes. Unknown journal, 2019.

5. (baribault2019usinghierarchicalbayesian pages 38-42): B Baribault. Using hierarchical bayesian models to test complex theories about the nature of latent cognitive processes. Unknown journal, 2019.

6. (baribault2023troubleshootingbayesiancognitive pages 1-6): Beth Baribault and A. Collins. Troubleshooting bayesian cognitive models. Psychological methods, Mar 2023. URL: https://doi.org/10.1037/met0000554, doi:10.1037/met0000554. This article has 28 citations and is from a highest quality peer-reviewed journal.

7. (baribault2023troubleshootingbayesiancognitive pages 40-43): Beth Baribault and A. Collins. Troubleshooting bayesian cognitive models. Psychological methods, Mar 2023. URL: https://doi.org/10.1037/met0000554, doi:10.1037/met0000554. This article has 28 citations and is from a highest quality peer-reviewed journal.

8. (baribault2023troubleshootingbayesiancognitive pages 49-53): Beth Baribault and A. Collins. Troubleshooting bayesian cognitive models. Psychological methods, Mar 2023. URL: https://doi.org/10.1037/met0000554, doi:10.1037/met0000554. This article has 28 citations and is from a highest quality peer-reviewed journal.

9. (baribault2023troubleshootingbayesiancognitive pages 53-57): Beth Baribault and A. Collins. Troubleshooting bayesian cognitive models. Psychological methods, Mar 2023. URL: https://doi.org/10.1037/met0000554, doi:10.1037/met0000554. This article has 28 citations and is from a highest quality peer-reviewed journal.

10. (baribault2023troubleshootingbayesiancognitive pages 6-9): Beth Baribault and A. Collins. Troubleshooting bayesian cognitive models. Psychological methods, Mar 2023. URL: https://doi.org/10.1037/met0000554, doi:10.1037/met0000554. This article has 28 citations and is from a highest quality peer-reviewed journal.

11. (evans2019responsetimedataprovide pages 23-24): Nathan J. Evans, William R. Holmes, and Jennifer S. Trueblood. Response-time data provide critical constraints on dynamic models of multi-alternative, multi-attribute choice. Psychonomic Bulletin &amp; Review, 26:901-933, Feb 2019. URL: https://doi.org/10.3758/s13423-018-1557-z, doi:10.3758/s13423-018-1557-z. This article has 71 citations.

12. (evans2019responsetimedataprovide pages 3-5): Nathan J. Evans, William R. Holmes, and Jennifer S. Trueblood. Response-time data provide critical constraints on dynamic models of multi-alternative, multi-attribute choice. Psychonomic Bulletin &amp; Review, 26:901-933, Feb 2019. URL: https://doi.org/10.3758/s13423-018-1557-z, doi:10.3758/s13423-018-1557-z. This article has 71 citations.

13. (evans2019responsetimedataprovide pages 30-32): Nathan J. Evans, William R. Holmes, and Jennifer S. Trueblood. Response-time data provide critical constraints on dynamic models of multi-alternative, multi-attribute choice. Psychonomic Bulletin &amp; Review, 26:901-933, Feb 2019. URL: https://doi.org/10.3758/s13423-018-1557-z, doi:10.3758/s13423-018-1557-z. This article has 71 citations.

14. (fengler2023likelihoodapproximationsfor pages 34-36): A Fengler. Likelihood approximations for bayesian analysis of sequential sampling models. Unknown journal, 2023.

15. (fengler2023likelihoodapproximationsfor pages 41-46): A Fengler. Likelihood approximations for bayesian analysis of sequential sampling models. Unknown journal, 2023.

16. (heathcote2019dynamicmodelsof pages 1-2): Andrew Heathcote, Yi-Shin Lin, Angus Reynolds, Luke Strickland, Matthew Gretton, and Dora Matzke. Dynamic models of choice. Behavior Research Methods, 51:961-985, Jun 2019. URL: https://doi.org/10.3758/s13428-018-1067-y, doi:10.3758/s13428-018-1067-y. This article has 192 citations and is from a domain leading peer-reviewed journal.

17. (heathcote2019dynamicmodelsof pages 6-7): Andrew Heathcote, Yi-Shin Lin, Angus Reynolds, Luke Strickland, Matthew Gretton, and Dora Matzke. Dynamic models of choice. Behavior Research Methods, 51:961-985, Jun 2019. URL: https://doi.org/10.3758/s13428-018-1067-y, doi:10.3758/s13428-018-1067-y. This article has 192 citations and is from a domain leading peer-reviewed journal.

18. (heck2016extendingmultinomialprocessing pages 18-19): Daniel W. Heck and Edgar Erdfelder. Extending multinomial processing tree models to measure the relative speed of cognitive processes. Psychonomic Bulletin &amp; Review, 23:1440-1465, Jun 2016. URL: https://doi.org/10.3758/s13423-016-1025-6, doi:10.3758/s13423-016-1025-6. This article has 95 citations.

19. (heck2016extendingmultinomialprocessing pages 6-8): Daniel W. Heck and Edgar Erdfelder. Extending multinomial processing tree models to measure the relative speed of cognitive processes. Psychonomic Bulletin &amp; Review, 23:1440-1465, Jun 2016. URL: https://doi.org/10.3758/s13423-016-1025-6, doi:10.3758/s13423-016-1025-6. This article has 95 citations.

20. (kyllonen2016useofresponse pages 13-16): Patrick Kyllonen and Jiyun Zu. Use of response time for measuring cognitive ability. Journal of Intelligence, 4:14, Nov 2016. URL: https://doi.org/10.3390/jintelligence4040014, doi:10.3390/jintelligence4040014. This article has 171 citations and is from a peer-reviewed journal.

21. (lee2011howcognitivemodeling pages 1-2): Michael D. Lee. How cognitive modeling can benefit from hierarchical bayesian models. Journal of Mathematical Psychology, 55:1-7, Feb 2011. URL: https://doi.org/10.1016/j.jmp.2010.08.013, doi:10.1016/j.jmp.2010.08.013. This article has 397 citations and is from a peer-reviewed journal.

22. (lee2011howcognitivemodeling pages 3-4): Michael D. Lee. How cognitive modeling can benefit from hierarchical bayesian models. Journal of Mathematical Psychology, 55:1-7, Feb 2011. URL: https://doi.org/10.1016/j.jmp.2010.08.013, doi:10.1016/j.jmp.2010.08.013. This article has 397 citations and is from a peer-reviewed journal.

23. (lee2011howcognitivemodeling pages 6-6): Michael D. Lee. How cognitive modeling can benefit from hierarchical bayesian models. Journal of Mathematical Psychology, 55:1-7, Feb 2011. URL: https://doi.org/10.1016/j.jmp.2010.08.013, doi:10.1016/j.jmp.2010.08.013. This article has 397 citations and is from a peer-reviewed journal.

24. (lee2018bayesianmethodsin pages 1-3): Michael D. Lee. Bayesian methods in cognitive modeling. Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience, pages 1-48, Mar 2018. URL: https://doi.org/10.1002/9781119170174.epcn502, doi:10.1002/9781119170174.epcn502. This article has 115 citations.

25. (lee2018bayesianmethodsin pages 3-5): Michael D. Lee. Bayesian methods in cognitive modeling. Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience, pages 1-48, Mar 2018. URL: https://doi.org/10.1002/9781119170174.epcn502, doi:10.1002/9781119170174.epcn502. This article has 115 citations.

26. (lee2018bayesianmethodsin pages 31-33): Michael D. Lee. Bayesian methods in cognitive modeling. Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience, pages 1-48, Mar 2018. URL: https://doi.org/10.1002/9781119170174.epcn502, doi:10.1002/9781119170174.epcn502. This article has 115 citations.

27. (lu2025hierarchicalbayesianaugmented pages 11-14): Zhong-Lin Lu, Shanglin Yang, and Barbara Anne Dosher. Hierarchical bayesian augmented hebbian reweighting model of perceptual learning. Journal of Vision, 25:9, Apr 2025. URL: https://doi.org/10.1167/jov.25.4.9, doi:10.1167/jov.25.4.9. This article has 0 citations and is from a domain leading peer-reviewed journal.

28. (lu2025hierarchicalbayesianaugmented pages 5-8): Zhong-Lin Lu, Shanglin Yang, and Barbara Anne Dosher. Hierarchical bayesian augmented hebbian reweighting model of perceptual learning. Journal of Vision, 25:9, Apr 2025. URL: https://doi.org/10.1167/jov.25.4.9, doi:10.1167/jov.25.4.9. This article has 0 citations and is from a domain leading peer-reviewed journal.

29. (molloy2019what’sina pages 1-5): M. Fiona Molloy, Matthew Galdo, Giwon Bahg, Qingfang Liu, and Brandon M. Turner. What’s in a response time?: on the importance of response time measures in constraining models of context effects. Decision, 6:171-200, Apr 2019. URL: https://doi.org/10.1037/dec0000097, doi:10.1037/dec0000097. This article has 21 citations and is from a peer-reviewed journal.

30. (murrow2024modelsandmethods pages 102-104): M Murrow. Models and methods for the computational study of human decision-making using choice-response time data. Unknown journal, 2024.

31. (murrow2024modelsandmethods pages 104-107): M Murrow. Models and methods for the computational study of human decision-making using choice-response time data. Unknown journal, 2024.

32. (murrow2024modelsandmethods pages 31-34): M Murrow. Models and methods for the computational study of human decision-making using choice-response time data. Unknown journal, 2024.

33. (murrow2024modelsandmethods pages 78-80): M Murrow. Models and methods for the computational study of human decision-making using choice-response time data. Unknown journal, 2024.

34. (ratcliff2015individualdifferencesand pages 46-49): Roger Ratcliff and Russ Childers. Individual differences and fitting methods for the two-choice diffusion model of decision making. Decision, 2:237-279, Oct 2015. URL: https://doi.org/10.1037/dec0000030, doi:10.1037/dec0000030. This article has 234 citations and is from a peer-reviewed journal.

35. (ratcliff2015individualdifferencesand pages 49-53): Roger Ratcliff and Russ Childers. Individual differences and fitting methods for the two-choice diffusion model of decision making. Decision, 2:237-279, Oct 2015. URL: https://doi.org/10.1037/dec0000030, doi:10.1037/dec0000030. This article has 234 citations and is from a peer-reviewed journal.

36. (ratcliff2015individualdifferencesand pages 53-56): Roger Ratcliff and Russ Childers. Individual differences and fitting methods for the two-choice diffusion model of decision making. Decision, 2:237-279, Oct 2015. URL: https://doi.org/10.1037/dec0000030, doi:10.1037/dec0000030. This article has 234 citations and is from a peer-reviewed journal.

37. (rouder2015thelognormalrace pages 1-3): Jeffrey N. Rouder, Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. The lognormal race: a cognitive-process model of choice and latency with desirable psychometric properties. Psychometrika, 80:491-513, Jun 2015. URL: https://doi.org/10.1007/s11336-013-9396-3, doi:10.1007/s11336-013-9396-3. This article has 99 citations and is from a domain leading peer-reviewed journal.

38. (rouder2015thelognormalrace pages 3-5): Jeffrey N. Rouder, Jordan M. Province, Richard D. Morey, Pablo Gomez, and Andrew Heathcote. The lognormal race: a cognitive-process model of choice and latency with desirable psychometric properties. Psychometrika, 80:491-513, Jun 2015. URL: https://doi.org/10.1007/s11336-013-9396-3, doi:10.1007/s11336-013-9396-3. This article has 99 citations and is from a domain leading peer-reviewed journal.
