# Task
Title: Computational learning rules and parameter estimation in normative control architectures

# Response
Prompt:
“Survey the last 15 years of computational models and empirical studies that specify learning algorithms for updating norm/goal weights or gating parameters in decision-control systems. Cover:

Bayesian updating of rule probabilities in hierarchical control (e.g. predictive coding)

Reinforcement-learning approaches (TD, actor-critic) that tune veto/gate thresholds based on reward or error signals

Hebbian / associative mechanisms that embed normative content via experience or imitation

Parameter-fitting protocols: Bayesian model-comparison, MCMC, hierarchical HDDM, or cross-validation on developmental or conflict-adaptation datasets

For each: model name, core learning equations, data/tasks used for fitting (e.g. Stroop adaptation, Rule-shift tasks), reported parameter ranges, and any neural correlates of the learning signals.”

This report surveys the last 15 years of computational models and empirical studies that specify learning algorithms for updating norm or goal weights and gating parameters in decision‐control systems. In particular, we review models that use Bayesian updating of rule probabilities in hierarchical control systems (including predictive coding frameworks), reinforcement‐learning approaches that tune veto or gate thresholds based on reward or error signals, and Hebbian/associative mechanisms that embed normative content through experience or imitation. We also discuss parameter‐fitting protocols—including Bayesian model-comparison techniques, Markov Chain Monte Carlo (MCMC) methods, hierarchical drift diffusion modeling (HDDM), and cross-validation procedures used on developmental and conflict‐adaptation datasets—and summarize empirical tasks, reported parameter ranges, and neural correlates of the learning signals.

I. Bayesian Updating in Hierarchical Control and Predictive Coding

A number of recent computational models have formalized hierarchical control as a Bayesian inference problem. In these frameworks, the brain is modeled as an inference machine that dynamically updates its beliefs about latent rules or cause–effect contingencies underlying observed events. For instance, early Bayesian hierarchical models such as the Hierarchical Gaussian Filter (HGF) and its related formulations explicitly derive update equations in which prediction errors are weighted by the precision of the underlying estimates. The core learning rule in these models takes the general form where the belief estimate at trial k is updated as
  m(k)₍ᵢ₎ = m(k–1)₍ᵢ₎ + η(k)₍ᵢ₎ε(k)₍ᵢ₎,
with the adaptive learning rate η(k)₍ᵢ₎ defined by precision ratios computed from both the prediction at the given hierarchical level and the bottom‐up inputs, and ε(k)₍ᵢ₎ representing the prediction error at level i. Such equations mathematically capture the essence of Bayesian updating in settings where the environment is volatile and internal representations must be modulated by changing levels of confidence (Diaconescu2014Inferringonthe pages 5-6). In the predictive coding framework, hierarchical generative models instantiate these Bayesian principles by positing that higher cortical layers generate predictions about lower-level representations and that only the residual error—the unexpected components—propagates upward. Notable works (Hosseini2005HierarchicalPredictiveCoding pages 1-3, Jiang2024DynamicPredictiveCoding pages 29-30) describe architectures in which the minimization of prediction error via gradient descent on free-energy functionals indirectly performs Bayesian updating of latent variables, effectively evolving rule probabilities or normative expectations. These models have been applied to sensory inference and sequence learning tasks, such as next-frame video prediction and auditory tone sequence tasks, where subject behavior or neural patterns can be explained by the modulation of internal representations as a function of confidence and surprise (Marino2022PredictiveCodingVariational pages 11-14). Empirical studies have further provided evidence that brain regions such as the inferior frontal cortex and parietal cortices encode these hierarchical prediction errors and confidence signals (Meyniel2017BrainNetworksfor pages 1-3, Meyniel2017BrainNetworksfor pages 7-8). Reported parameter ranges are often derived from fitting the variance of the prediction distributions and the derived learning rates, with some studies indicating that learning rate adjustments occur on the order of 0.1 in stable conditions but are modulated upward rapidly in volatile contexts. These Bayesian and predictive coding models not only capture behavioral adaptation in rule-shift and conflict-adaptation tasks but also offer a normative account of how rule probabilities and associated goal weights are continuously updated by integrating bottom-up sensory evidence with top-down priors.

II. Reinforcement-Learning Approaches for Tuning Veto/Gate Thresholds

Reinforcement learning (RL) models, particularly those based on temporal difference (TD) learning and actor–critic architectures, have been widely deployed to explain normative decision-control through the tuning of gate or veto thresholds. In these frameworks, agents maintain parallel estimates of action values and update these based on the discrepancy between expected and received rewards. A classic example comes from the Opponent Actor Learning (OpAL) model, which explicitly models two separate sets of actor values—one representing direct “Go” pathways and another representing indirect “NoGo” pathways—and updates them using dopamine-dependent reward prediction errors. The core update equations for this model are given by:
  AG(s, a) ← AG(s, a) + αG · AG(s, a) · δt  and  AN(s, a) ← AN(s, a) − αN · AN(s, a) · δt,
where AG(s, a) and AN(s, a) denote the Go and NoGo action values respectively, αG and αN denote learning rates for the respective pathways, and δt is the dopaminergic prediction error signal that reflects the difference between received and expected rewards (Guiomar2023Parallelloopsof pages 63-67). Action selection is typically performed via a softmax function over a weighted difference between these two pathways, ensuring that a veto signal from the indirect pathway can dynamically adjust the threshold for action execution. Empirical data supporting such models have been derived from motor skill learning tasks such as the rotarod, and from stay-or-leave foraging experiments in rodents. In these tasks, mice have exhibited decision thresholds that adjust in response to unexpected reward magnitudes, which are well captured by models that integrate a leaky averaging of past rewards (Shuvaev2020RLearninginActorCritic pages 6-7, Shuvaev2020RLearninginActorCritic pages 9-11). Furthermore, TD learning models in continuous time have been extended to spiking neural networks, where the TD error modulates synaptic plasticity akin to dopamine-modulated STDP, thus providing a neural substrate for controlling gating parameters in the brain (Frémaux2013ReinforcementLearningUsing pages 1-2, Frémaux2013ReinforcementLearningUsing pages 2-3). Reported parameter ranges in these models often indicate learning rates (α) on the order of 0.1–0.4 for various behavioral tasks, with additional parameters such as inverse temperature factors (β) governing the stochasticity of choice being calibrated to match behavioral variability observed in experiments. Neural correlates for these RL models have been found in dopaminergic neurons within the ventral tegmental area (VTA) and in striatal circuits responsible for processing reward prediction errors (Ballard2018BeyondRewardPrediction pages 25-29, Ballard2018BeyondRewardPrediction pages 29-32), validating the link between normative TD learning signals and action gating thresholds in the brain.

III. Hebbian and Associative Mechanisms Embedding Normative Content

In addition to Bayesian and reinforcement learning models, another class of computational models emphasizes Hebbian and associative learning mechanisms to embed normative content—such as rule and goal representations—via experience or imitation. These models rely on neural plasticity rules that potentiate synapses when pre- and postsynaptic activity are coincident, modulated by neuromodulatory signals that can gate the sign and magnitude of plastic changes. For instance, dis-inhibitory cortical microcircuits have been proposed to control the sign of synaptic plasticity, offering a mechanism whereby local Hebbian changes can be modulated by higher-order learning signals (Rossbroich2023DisinhibitoryNeuronalCircuits pages 1-3). In these models, normative content is gradually embedded through repeated exposures to task-relevant patterns, with reward signals or error signals serving as a third factor in the “three-factor” Hebbian rule. The basic formulation for such rules is given by:
  Δwᵢⱼ = α_A · f(δ_t) · preⱼ · postᵢ,
where α_A represents the learning rate for the Hebbian update, f(δ_t) is a nonlinear transfer function that modulates synaptic changes based on the prediction error δ_t, and preⱼ and postᵢ represent pre- and postsynaptic neuronal activities, respectively (Guiomar2023Parallelloopsof pages 90-94). These underlying rules are consistent with experimental findings showing that dopamine can selectively potentiate synaptic connections in direct pathways while depressing those in indirect pathways. Furthermore, such Hebbian rules have been extended into more complex network architectures that implement imitation learning by adjusting internal representations based on observed normative behavior, thus embedding goal weights in a distributed fashion across cortical networks. Although explicit parameter ranges for Hebbian models are typically derived from experimental measures of synaptic plasticity (often on the scale of a few percent changes per pairing event), their integration into hierarchical control systems establishes the foundation for normative decision-making through associative learning as well.

IV. Parameter-Fitting Protocols

A critical component of all these computational frameworks is the rigorous estimation of model parameters and the comparison of competing models against behavioral and neural data. Bayesian model-comparison techniques have been widely used to evaluate normative control models because they offer a principled way to balance model fit and complexity. Hierarchical Bayesian models, in particular, lend themselves to the simultaneous estimation of individual and group-level parameters while controlling for overfitting. For example, the hBayesDM package provides tools for hierarchical Bayesian analysis of reinforcement learning and decision-making tasks, allowing for the extraction of latent trial-by-trial variables such as prediction errors, learning rates, and inverse temperatures (Ahn2017RevealingNeurocomputationalMechanisms pages 1-2, Ahn2017RevealingNeurocomputationalMechanisms pages 11-12). Parameter ranges reported in these studies are typically constrained by prior experimental knowledge; for example, learning rates are commonly restricted to the interval (0, 1), inverse temperature parameters may range from 0 to 10, and parameters related to norm sensitivity (such as envy or guilt in social exchange tasks) are similarly bounded to reflect psychological plausibility (Ahn2017RevealingNeurocomputationalMechanisms pages 18-20). In other approaches, model fitting is performed using Markov Chain Monte Carlo (MCMC) sampling methods, which have the advantage of providing full posterior distributions over parameters, thereby quantifying uncertainty in the estimates. Cross-validation techniques, such as leave-one-out information criteria (LOOIC) or widely applicable information criterion (WAIC), have also been applied, particularly to developmental and conflict-adaptation datasets, where out-of-sample predictive performance is critical for validating normative models (Ahn2017RevealingNeurocomputationalMechanisms pages 2-4, Ahn2017RevealingNeurocomputationalMechanisms pages 7-9). These parameter-fitting protocols have been instrumental in quantitatively validating normative learning models by matching model predictions with empirical data gathered from tasks such as the Stroop adaptation, rule-shift tasks, and foraging paradigms, and by identifying neural correlates of model-derived latent variables within frontostriatal and parietal networks (Meyniel2017BrainNetworksfor pages 8-9, Ritz2018AControlTheoretic pages 19-21).

V. Integration of Normative Control Signals Across Models

Taken together, the surveyed literature demonstrates that different computational approaches converge on a common framework: normative control is achieved by dynamically updating internal representations of rules, goals, or gate thresholds via learning signals that reflect discrepancies between expected and observed outcomes. Bayesian models operate by adjusting the weights of rule probabilities in hierarchical predictive coding networks, reconciling top-down priors with bottom-up sensory input through precision-weighted prediction errors. Reinforcement learning models, on the other hand, tune interactive gate thresholds (or veto signals) in actor–critic systems via temporal difference errors modulated by dopamine, thereby balancing action facilitation and suppression. Concurrently, Hebbian and associative learning mechanisms modulated by neuromodulatory signals work to embed normative content over the long term, effectively “hard-wiring” rule-based control into synaptic architecture. Each of these models has been rigorously tested by fitting to behavioral and neural datasets obtained from tasks that probe rule learning, conflict adaptation, and decision-making under uncertainty. Moreover, the parameter estimation techniques applied—from hierarchical Bayesian inference to cross-validation and MCMC sampling—ensure that the inferred learning rates, sensitivity parameters, and gating thresholds are not only statistically robust but also biologically plausible. Neural correlates supporting these computational constructs have been consistently found across studies, implicating regions such as the dorsal and ventral striatum, anterior cingulate cortex, and the inferior frontal gyrus, which have been shown to encode both reward prediction errors and confidence-weighted updates (Guiomar2023Parallelloopsof pages 165-169, Neftci2019ReinforcementLearningIn pages 3-5).

VI. Discussion and Future Directions

The intersection of Bayesian inference, reinforcement learning, and Hebbian mechanisms in normative control architectures represents a rapidly evolving field. While Bayesian and predictive coding models offer an elegant normative framework for rule updating in hierarchical control systems, they also often require complex approximations to remain computationally tractable. Reinforcement learning models incorporate biologically realistic mechanisms such as dopaminergic modulation and STDP, yet must contend with the variability inherent in real-world behavior. Hebbian models, despite their simplicity and direct correspondence to synaptic plasticity data, may poorly account for rapid learning observed in volatile environments unless combined with modulatory signals that provide a meta-learning context. In practice, hybrid models that integrate aspects of all three theoretical approaches seem best poised to explain the full spectrum of normative decision-control signals observed in both behavioral and neural data. Ongoing research is also directed toward more efficient parameter-fitting procedures that leverage hierarchical and Bayesian estimation frameworks, thereby scaling these methods to larger and more heterogeneous datasets such as developmental studies and conflict-adaptation paradigms. Future work may further link computational parameters to precise neurophysiological measures—for instance, the dynamic modulation of receptor subtypes in striatal neurons or shifts in oscillatory power in prefrontal circuits—thus bridging the gap between abstract computational theories and explicit biological mechanisms. These advances are expected to yield more unified models of decision control that not only fit observed behavior but also predict intervention outcomes in clinical or educational settings.

VII. Conclusion

The surveyed literature over the past 15 years reveals a rich tapestry of computational models that specify learning algorithms for updating rule probabilities, goal weights, and gating parameters in normative control architectures. Bayesian updating models grounded in predictive coding frameworks describe how hierarchical inference is achieved through precision-weighted prediction errors, while reinforcement learning approaches—exemplified by TD and actor–critic architectures such as the OpAL model—detail how error or reward signals modulate dynamic veto thresholds. Complementarily, Hebbian and associative learning mechanisms, especially when modulated by neuromodulators, provide a substrate for embedding normative content by adjusting synaptic weights through experience or imitation. State-of-the-art parameter-fitting protocols, which include Bayesian model-comparison techniques, MCMC sampling, and cross-validation approaches as applied to tasks like Stroop adaptation and rule-shift paradigms, have further enabled these models to be rigorously quantified and validated with neurobiological data. Collectively, these efforts represent a significant advance in our theoretical and empirical understanding of the computational bases of decision control in the brain, and they continue to lay the groundwork for future research that bridges cognitive theory, neural implementation, and clinical application (Guiomar2023Parallelloopsof pages 63-67, Shuvaev2020RLearninginActorCritic pages 6-7, Ahn2017RevealingNeurocomputationalMechanisms pages 1-2, Rossbroich2023DisinhibitoryNeuronalCircuits pages 1-3, Meyniel2017BrainNetworksfor pages 1-3, Frémaux2013ReinforcementLearningUsing pages 1-2, Hosseini2005HierarchicalPredictiveCoding pages 1-3, Ahn2017RevealingNeurocomputationalMechanisms pages 11-12).

In summary, the integration of Bayesian, reinforcement, and Hebbian learning rules in normative control systems has provided robust models capable of explaining adaptive behavior and corresponding neural dynamics. These models not only offer insights into the computational mechanisms underlying decision-making but also pave the way for future interdisciplinary studies that may leverage these frameworks to diagnose and treat dysfunctions in normative control observed in clinical populations.


## References
1. [tscshantz2023hybridpredictivecoding](https://doi.org/10.1371/journal.pcbi.1011280)
2. [ballard2018beyondrewardprediction](https://doi.org/10.1101/115253)
3. [hosseini2005hierarchicalpredictivecoding](https://doi.org/10.48550/arxiv.2005.03230)
4. [kanai2015cerebralhierarchiespredictive](https://doi.org/10.1098/rstb.2014.0169)
5. [dagar2023computationalmodelingof](https://theses.hal.science/tel-04301585/document)
6. [meyniel2020braindynamicsfor](https://doi.org/10.1371/journal.pcbi.1007935)
7. [hohwy2017priorsinperception](https://doi.org/10.1016/j.concog.2016.09.004)
8. [meyniel2017brainnetworksfor](https://doi.org/10.1073/pnas.1615773114)
9. [millidge2022predictivecodingtowards](https://doi.org/10.48550/arxiv.2202.09467)
10. [millidge2107predictivecodinga](https://doi.org/10.48550/arxiv.2107.12979)
11. [guo2019hierarchicalbayesianinference](https://doi.org/10.1109/tcyb.2017.2768554)
12. [ahn2017revealingneurocomputationalmechanisms](https://doi.org/10.1162/cpsy_a_00002)
13. [ritz2018acontroltheoretic](https://doi.org/10/1405/28928)
14. [diaconescu2014inferringonthe](https://doi.org/10.1371/journal.pcbi.1003810)
15. [marino2022predictivecodingvariational](https://doi.org/10.1162/neco_a_01458)
16. [iglesias2013hierarchicalpredictionerrors](https://doi.org/10.1016/j.neuron.2013.09.009)
17. [rule2022selfhealingcodeshow](https://doi.org/10.1101/2021.03.08.433413)
18. [jiang2024dynamicpredictivecoding](https://doi.org/10.1371/journal.pcbi.1011801)
19. [eckstein2020computationalevidencefor](https://doi.org/10.1073/pnas.1912330117)
20. [lansner2401benchmarkinghebbianlearning](https://doi.org/10.48550/arxiv.2401.00335)
21. [tyulmankov2412computationalmodelsof](https://doi.org/10.48550/arxiv.2412.05501)
22. [chalvidal2023learninghigherorderfunctions](https://theses.hal.science/tel-04502217/file/2023TOU30256a.pdf)
23. [millidge2022atheoreticalframework](https://doi.org/10.48550/arxiv.2207.12316)
24. [frank2012mechanismsofhierarchical](https://doi.org/10.1093/cercor/bhr114)
25. [rossbroich2023disinhibitoryneuronalcircuits](https://doi.org/10.48550/arxiv.2310.19614)
26. [aitchison2017withorwithout](https://doi.org/10.1016/j.conb.2017.08.010)
27. [weilnhammer2018theneuralcorrelates](https://doi.org/10.1523/jneurosci.2901-17.2018)
28. [jiang2112predictivecodingtheories](https://doi.org/10.48550/arxiv.2112.10048)
29. [jiang2014bayesianmodelingof](https://doi.org/10.1016/j.neubiorev.2014.06.001)
30. [golkar2022constrainedpredictivecoding](https://doi.org/10.48550/arxiv.2210.15752)
31. [shuvaev2020rlearninginactorcritic](https://proceedings.neurips.cc/paper_files/paper/2020/file/da97f65bd113e490a5fab20c4a69f586-Paper.pdf)
32. [guiomar2023parallelloopsof](https://run.unl.pt/bitstream/10362/168546/2/PhD_Thesis_GoncaloGuiomar.pdf)
33. [fremaux2013reinforcementlearningusing](https://doi.org/10.1371/journal.pcbi.1003024)
34. [neftci2019reinforcementlearningin](https://doi.org/10.1038/s42256-019-0025-4)
35. [wiecki2015computationalpsychiatrycombining](https://ski.clps.brown.edu/papers/wiecki_phd_thesis.pdf)
36. [wong2021neuralmechanismsof](https://ora.ox.ac.uk/objects/uuid:17d85aa4-c70a-4a2e-b755-9719fec3933c/files/d1831ck242)
37. [zappacosta2018generaldifferentialhebbian](https://doi.org/10.1371/journal.pcbi.1006227)
38. [ciria2021predictiveprocessingin](https://doi.org/10.1162/neco_a_01383)
39. [besold2021chapter1.neuralsymbolic](https://arxiv.org/pdf/1711.03902)
